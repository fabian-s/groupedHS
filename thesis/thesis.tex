
\documentclass[12pt,letterpaper]{article} 
\usepackage[letterpaper,left=3.5cm,right=3.5cm, top=3.0cm, bottom=3.0cm, footnotesep=1.0cm]{geometry} 
\usepackage{mathptmx}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\usepackage[onehalfspacing]{setspace} 
\usepackage{fancyhdr} 
\usepackage{relsize}

\usepackage[bottom]{footmisc}
\usepackage{tabularx}
\usepackage{mathtools}
\pagestyle{empty}        


\usepackage{booktabs}    
\usepackage{natbib}      
\usepackage{bibentry}   

\usepackage{acronym}
\usepackage{multicol}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage[usenames,dvipsnames]{color}

\lstset{language=R,
  basicstyle=\small\ttfamily,
  stringstyle=\color{ForestGreen},
  otherkeywords={0,1,2,3,4,5,6,7,8,9},
  morekeywords={TRUE,FALSE},
  deletekeywords={data,frame,length,as,character},
  keywordstyle=\color{blue},
  commentstyle=\color{ForestGreen},
}


\begin{document}

\begin{center}\uppercase{Ludwig-Maximilians-University Munich}\end{center}
\begin{center}\uppercase{Department of Statistics}\end{center}

\vspace{3cm}

\title{Variable selection using Grouped Horseshoe priors}
\date{\vspace{-5ex}}
{\let\newpage\relax\maketitle}
\thispagestyle{empty}


\begin{center}
  \begin{large}
    \begin{Large}
      Bachelor's Thesis\\
    \end{Large}
    Department of Statistics  \\
  \end{large}
\end{center}
\begin{center}
  Author:\\
  \begin{large}
    Tobias Pielok\\
  \end{large}
\end{center}
\vspace{1cm}
\begin{center}
  \begin{large}
    Supervisor: Dr. Fabian Scheipl
  \end{large}
\end{center}

\begin{center}  \begin{large}
    Submission date: \\%\date{\today} \\
  \end{large}
\end{center}

\vspace{1,5cm}

\begin{center}
  \begin{large}
    \author{Tobias Pielok, B.Sc. (TUM)}\\
 \end{large}
  Marchgrabenplatz 5\\ 
  80805 Munich\\ 
  \url{t.pielok@campus.lmu.de}\\
  Matrikelnr.:  11381351\\
\end{center}


\newpage
\setcounter{page}{1}
\tableofcontents
\newpage


\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt} 
\section*{Notation and Symbols}

\begin{tabular}{lll}
$\mathcal{B}_\mathcal{M}\bigotimes\mathcal{B}_\mathcal{N}$ & & product-$\sigma$-algebra generated by $\{M\times N :\; M\in\mathcal{B}_\mathcal{M}, N\in\mathcal{B}_\mathcal{N} \}$ 
\end{tabular}

\pagebreak
\section{Introduction}
\pagebreak

\section{Basic Concepts}
The foundation of Bayesian Horseshoe priors is based on the inference of the posterior distribution of the parameters, which can be attained by combining the prior knowledge of the parameters and the likelihood of the data, and will be introduced in section \ref{sec:BayInf}. In order of carrying out this inference it is necessary to sample from the posterior distribution.
In most cases this posterior distribution can be neither expressed in an analytical form nor are there standard samplers for it. Hence the HMC Method and its derivative NUTS will be shown in section \ref{sec:HMC}, with which it is possible to sample from the posterior distribution even in a high dimensional case. To express in this Bayesian framework the structure of grouped variables, on which the grouped Horseshoe priors will be later used in section \ref{sec:gHSP},  additive models will be introduced in section \ref{sec:AddMod}.
\subsection{Bayesian inference}
\label{sec:BayInf}
In Bayesian statistics the inference is carried out on the posterior distribution of a parameter $\tau$, after taking into account the realization of data $x$. In order of defining this posterior distribution thoroughly some rigorous definitions are needed:
Let $\mathcal{X}$ and $\mathcal{T}$ denote the \textit{sample space} and \textit{parameter space}, i.e. $\tau \in \mathcal{T}$ and $x \in \mathcal{X}$. Their $\sigma$-algebras will be called respectively $\mathcal{B}_\mathcal{X}$ and $\mathcal{B}_\mathcal{T}$. Also let the family of \textit{sampling distribution} $\mathcal{P} := \{P_\theta :\;\theta \in \mathcal{T}\}$, where $P_\theta$ is a probability measure over $(\mathcal{X}, \mathcal{B}_\mathcal{X})$. With these definitions a general \textit{statistical experiment} can be defined as $(\mathcal{X}, \mathcal{B}_\mathcal{X}, \mathcal{P})$ and for any given probability measure $\mu$ on $(\mathcal{T}, \mathcal{B}_\mathcal{T})$ a \textit{Bayesian experiment} can be identified with $(\mathcal{T}\times\mathcal{X},  \mathcal{B}_\mathcal{T}\bigotimes\mathcal{B}_\mathcal{X}, \Pi_{\mu,\mathcal{P}})$, where $\Pi_{\mu,\mathcal{P}}$ is the joint distribution of parameter-observation, s.t. $\forall T \in \mathcal{B}_\mathcal{T}, X \in \mathcal{B}_\mathcal{X}$:
\begin{equation}
\label{eq:likprio}
\Pi_{\mu,\mathcal{P}}(T,X) = \int_T P_\theta(X)\mu(d\theta ). 
\end{equation} 
The measure $\mu$ will be called the \textit{prior distribution} of the parameter. The \textit{predictive distribution} of the observations $P_{\mu,\mathcal{P}}$ is the marginal distribution of $\Pi_{\mu,\mathcal{P}}$ on $(\mathcal{X}, \mathcal{B}_\mathcal{X})$, s.t. $\forall X \in \mathcal{B}_\mathcal{X}: $
\begin{equation}
P_{\mu,\mathcal{P}}(X) = \Pi_{\mu,\mathcal{P}}(\mathcal{T},X).
\label{eq:pred}
\end{equation}
Eq. (\ref{eq:likprio}) can be rewritten with the predictive distribution (\ref{eq:pred}) and the family of \textit{posterior distributions} $\{\mu_\mathcal{P}(\cdot |\; x):\; x\in\mathcal{X}\}$, s.t. $\forall T \in \mathcal{B}_\mathcal{T}, X \in \mathcal{B}_\mathcal{X}$
\begin{equation}
\label{eq:post}
\Pi_{\mu,\mathcal{P}}(T,X) = \int_X \mu_\mathcal{P}(T|\; x) P_{\mu,\mathcal{P}}(dx ). 
\end{equation} 
Under the assumption that the Bayesian experiment $(\mathcal{T}\times\mathcal{X},  \mathcal{B}_\mathcal{T}\bigotimes\mathcal{B}_\mathcal{X}, \Pi_{\mu,\mathcal{P}})$ is dominated in the Bayesian setting (if not other stated, this will be assumed for the rest of the thesis) it can be shown (see [\cite{interbayes}]), that a posterior distribution for given $x$ can also be expressed with the Radon-Nikodym derivative, s.t. $\forall T \in \mathcal{B}_\mathcal{T}:$
\begin{equation}
\label{eq:dpost}
 \mu_\mathcal{P}(T|\; x) = \frac{\int_T \frac{dP_\theta}{d\lambda}(x)\mu(d\theta)}{\int_\mathcal{T} \frac{dP_\theta}{d\lambda}(x)\mu(d\theta)},
\end{equation} 
where $(\mathcal{X}, \mathcal{B}_\mathcal{X}, \mathcal{P})$ is dominated by a finite measure $\lambda$. The function $\frac{dP_\theta}{d\lambda}$ is called \textit{likelihood function}. 
%Why? -> inference%
\\An adequate choice of a point estimate to determine the location of the parameter $\theta$ depends on the underlying loss function. If a quadratic loss function $L$ is chosen, i.e. $L(\theta, \hat{\theta}) = ||\theta - \hat{\theta}||_2^2$, the expected loss $\mathbb{E}_{\theta \sim \mu} (L(\theta, \hat{\theta}))$ is minimized by the \textit{posterior mean} $
	\hat{\theta} = E_{\theta |x \sim \mu_\mathcal{P}}(\theta |\; x)$, if it exists (see [\cite{statdec}]). 
\\ A $100(1-\alpha)\%$ \textit{credible set} is a set $C \subset \mathcal{T}$, s.t.
\begin{equation}
1 - \alpha \leq P(C| \;x ) = \mu_\mathcal{P}(C|\; x).
\end{equation}
Which means that the parameter $\theta$ has the subjective probability of $(1-\alpha)$ that $\theta \in C$.
Because $C$ is not unique, additional constraints can be imposed on the solution. If the size of $C$, which can be defined as  $S(C) := \mu(C)$, is minimized one gets under mild conditions the  $100(1-\alpha)\%$  HDP credible set  as described in [\cite{statdec}], if there exists a posterior density $p_{\theta|\;x}$.
The  $100(1-\alpha)\%$ HDP credible set is defined as  
\begin{equation}
C = \{\theta \in \mathcal{T}: \; p_{\theta|\;x}(\theta|\; x) \geq k(\alpha)\},
\end{equation}
where $k(\alpha)$ is the largest constant, s.t. $C$ is a $100(1-\alpha)\%$ credible set.
\subsection{HMC and NUTS}

\label{sec:HMC}
\subsection{Bayesian approach to Additive Models}
\label{sec:AddMod}
\pagebreak

\section{Bayesian Horseshoe}
\subsection{Mathematical foundations}
\subsection{Hyperprior choice for global shrinkage parameter}
\subsection{Extending Horseshoe prior to the case of grouped variables}
\label{sec:gHSP}
\subsection{Practical aspects}
\pagebreak

\section{Simulations}
\subsection{Settings}

\subsection{Szenario 1}
\subsubsection{Data generation}
\subsubsection{Results}
\subsection{Szenario 2}
\subsubsection{Data generation}
\subsubsection{Results}

\subsection{Summary}
\pagebreak

\section{Benchmarking}

\subsection{Regression}
\subsubsection{Data description}
\subsubsection{Comparisions and Results}
\subsection{Classification}
\subsubsection{Data description}
\subsubsection{Comparisions and Results}

\subsection{Summary}

\pagebreak

\section{Conclusions}
\pagebreak
\section{Appendix}
\subsection{Other variable selection methods}
\subsubsection{Spike-and-slab variable selection}
\subsubsection{Feature selection using LASSO}
\pagebreak
\subsection{Code}
\pagebreak

\pagestyle{fancy}
\bibliographystyle{apalike}
\bibliography{thesis} 

\nocite{*} 
\addcontentsline{toc}{section}{Bibliography}
\clearpage

\listoffigures
\addcontentsline{toc}{section}{List of Figures}
\clearpage


\section*{List of Abbreviations}
\addcontentsline{toc}{section}{List of Abbreviations}
\begin{multicols}{2}
  \begin{acronym}[abr]
    \acro{HMC}{Hamiltonian Monte Carlo}
    \acro{NUTS}{No-U-Turn Sampler}    
    \acro{HDP}{highest posterior density}       
    \acro{MCMC}{Markov Chain Monte Carlo}
  \end{acronym}
\end{multicols}



\pagebreak
\subsection*{Statement}
\label{erklaerung}
\vspace*{0.5cm}
I hereby declare that this thesis is my own original work and that all sources have been acknowledged.\\[1.0cm]
Munich, \today \\[2.0cm]
\rule{6.0cm}{0.4pt} \\
Tobias Pielok


\end{document}
